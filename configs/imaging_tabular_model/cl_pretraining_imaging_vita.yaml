general:
  seed: 1
  wandb_disabled: "false"
  wandb_run_id: 
  resume_training: False
  resume_ckpt_path: ""


trainer:
  max_epochs: 2000
  check_val_every_n_epoch: 5
  devices: 1


data:
  general_data:
    train_num_per_epoch: 42000 # Must to be the integer multiple of batch size for multigpus
    num_train: 42000
    num_val: 1500
    num_test: 1000
    num_workers: 32
    batch_size: 256
    dataset_cls: ImagingAllAXTabular

  imaging_data:
    ignore_phenotype_tabular: True
    cmr_path_pickle_name: "cmr_paths.pkl"
    sax_slice_num: 6 # 6 for 3D, 1 for 2D
    image_size: [128, 128]
    frame_to_keep: 5
    augment: True

  tabular_data:
    tabular_data: "input_tab.csv"
    raw_tabular_data: "raw_tab.csv"
    corruption_rate: 0.3
    tab_augment: False
  
  validation_data:
    validation_form: "classification"
    # selected_cols: 
    labels: "labels_CAD.csv" # the csv file of the disease labels for the validation contrastive learning
    selected_cols: 


module:
  task_idx: 0 # 0: pretraining, 1: reconstruction
  module_idx: 0 # SimCLR: 0
  

  imaging_hparams:
    load_imaging_encoder: True
    freeze_imaging_encoder: False
    imaging_ckpt_path: "checkpoints_imaging/pretrain_mae_allax.ckpt" # the ckpt of pretrained imaging model
    patch_embed_cls: "PatchEmbed" # PatchEmbed_Spatial, PatchEmbed
    patch_size: [5, 8, 8]
    pixel_unshuffle_scale: 1
    circular_pe: False
    use_enc_pe: True
    mask_loss: True
    shift_size: [0, 0 ,0]
    mask_type: "random"
    mask_ratio: 0.5
    
    enc_embed_dim: 1025
    enc_depth: 6
    enc_num_heads: 5
    projection_dim: 128
    grad_checkpointing: True
  
  tabular_hparams:
    load_tabular_encoder: False
    freeze_tabular_encoder: False

    corruption_rate: 0.3
    one_hot: True
    eval_one_hot: True

    tabular_encoder_type: TabularViT
    embedding_dim: 1025
    encoder_num_heads: 5
    encoder_num_layers: 2
    projector_num_layers: 1
    init_strat: kaiming
    projection_dim: 128 # = imaging_hparams.projection_dim
    grad_checkpointing: True

  training_hparams:
    avg_token: True
    
    # Logging
    train_log_rate: 2
    val_log_rate: 2
    log_images: False

    # Criterions
    num_classes: 2
    temperature: 0.1
    lambda_0: 0.5

    # Optimization
    lr: 3.e-3
    weight_decay: 1.e-4
    scheduler: "anneal"
    warmup_epochs: 10
    anneal_max_epochs: 200
